{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-document summarization using Llama2 (Clustering + Graph RAG) ðŸ¦™"
      ],
      "metadata": {
        "id": "nYLcvkvO5ZfX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6xmvw2zJI96"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117 --upgrade\n",
        "!pip install -q langchain einops accelerate transformers scipy\n",
        "!pip install -q xformers sentencepiece sentence-transformers pypdf\n",
        "!pip install -q llama-index==0.7.21 llama_hub==0.0.19 openai\n",
        "!pip install -i https://test.pypi.org/simple/ bitsandbytes\n",
        "!pip install glob2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, GenerationConfig\n",
        "import huggingface_hub\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "huggingface_hub.notebook_login()"
      ],
      "metadata": {
        "id": "ACzUOwmYJd1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downloading the model and Text-generatioin pipeline"
      ],
      "metadata": {
        "id": "ihOME7pF5MyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(device_type, model_id):\n",
        "\n",
        "    print(f\"Loading Model: {model_id}, on: {device_type}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_id, cache_dir=\"./model/\"\n",
        "        )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        cache_dir=\"./model/\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True,\n",
        "        load_in_8bit=True,\n",
        "        )\n",
        "\n",
        "    generation_config = GenerationConfig.from_pretrained(model_id)\n",
        "\n",
        "    # Create a pipeline for text generation\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=4096,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.15,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "\n",
        "    local_llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature': 0})\n",
        "    print(\"Local LLM Loaded\")\n",
        "\n",
        "    return tokenizer, local_llm"
      ],
      "metadata": {
        "id": "PQB6RYnPJwke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SHOW_SOURCES = True"
      ],
      "metadata": {
        "id": "j4SwUGznKS79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, LLM = load_model(device, \"krthk/llama-2-7b-chat-finetuned\") #Huggingface model id"
      ],
      "metadata": {
        "id": "PH0waU_gKZUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dealing with the documents (Text extraction + clustering + adjusting chunk tokens)"
      ],
      "metadata": {
        "id": "48MvKWU25j6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "W1SA1lNDNDb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents_and_chunks(directory): #Takes the docs directory and returns documents\n",
        "\n",
        "  documents = []\n",
        "  for item_path in glob(directory + \"*.pdf\"):\n",
        "      loader = PyPDFLoader(item_path)\n",
        "      documents.extend(loader.load())\n",
        "\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = 1024,\n",
        "      chunk_overlap  = 128,  # similar token len in overlap of text between chunks\n",
        "      is_separator_regex = False,\n",
        "  )\n",
        "\n",
        "  raw_text = \"\"\n",
        "  text_chunks = []\n",
        "  for doc in documents:\n",
        "    raw_text += doc.page_content\n",
        "    text_chunks.append(doc.page_content)\n",
        "\n",
        "  return text_chunks, raw_text\n",
        "\n",
        "def calc_tokens(text):\n",
        "  return len(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "id": "6Dh0SkRJNLC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks, raw_text = load_documents_and_chunks(\"/content/documents/\")"
      ],
      "metadata": {
        "id": "ZWAqQ59GOjKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in chunks:\n",
        "  print(calc_tokens(i))"
      ],
      "metadata": {
        "id": "h7H88LyIE4jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "id": "GqyENwB3QSXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[10]"
      ],
      "metadata": {
        "id": "UwVNIwKJQZwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2') #Embedding model\n",
        "\n",
        "def cluster_sentences(sentences, distance_threshold=1.3): # AgglomerativeClustering for vector clustering\n",
        "    sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "    clustering_model = AgglomerativeClustering(distance_threshold=distance_threshold, n_clusters=None, linkage='ward')\n",
        "    clustering_model.fit(sentence_embeddings)\n",
        "\n",
        "    clustered_sentences = {}\n",
        "    for sentence_id, cluster_id in enumerate(clustering_model.labels_):\n",
        "        if cluster_id not in clustered_sentences:\n",
        "            clustered_sentences[cluster_id] = []\n",
        "        clustered_sentences[cluster_id].append(sentences[sentence_id])\n",
        "\n",
        "    return [cluster for cluster in clustered_sentences.values()]"
      ],
      "metadata": {
        "id": "GCO1jn-APlyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusters = cluster_sentences(chunks)"
      ],
      "metadata": {
        "id": "1A8Oq3YQP5zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in clusters:\n",
        "  if isinstance(i, list):\n",
        "    print(\"List length:\", len(i))\n",
        "  else:\n",
        "    print(\"String\")"
      ],
      "metadata": {
        "id": "3odBjkFBWqcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In a cluster where `n` chunks exist, each containing a certain number of tokens, the combined token size of the cluster is represented as `(tokens[0] + tokens[1] + ... + tokens[n])`. Occasionally, this total surpasses the input token limit of the Large Language Model (LLM), making it challenging for digestion. To address this, the clusters are redistributed, imposing a token limit for each cluster. The advantage of having similar information in each cluster ensures that the choice of elements within a cluster is inconsequential, as they consistently exhibit similarity.\n",
        "\n",
        "âœ… Therefore, achieving clusters of similar information which can fit into the LLM\n"
      ],
      "metadata": {
        "id": "uoiUHjEC6V-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def redistribution(listoflistofchunks):\n",
        "  limit = 2048\n",
        "  redistributed = []\n",
        "  for index, chunks in enumerate(listoflistofchunks):\n",
        "    tokens = calc_tokens(\"\\n\".join(chunks))\n",
        "\n",
        "    print(index, tokens)\n",
        "\n",
        "    if tokens < limit:\n",
        "      redistributed.extend([chunks])\n",
        "    else:\n",
        "      print(f\"chunk {index} of tokens {tokens} is splitted into {math.ceil(tokens/limit)} parts\")\n",
        "      partitionlist = np.array_split(chunks, math.ceil(tokens/limit))\n",
        "      # res = [list(x) for x in partitionlist]\n",
        "      redistributed.extend([list(x) for x in partitionlist])\n",
        "  return redistributed"
      ],
      "metadata": {
        "id": "586606GvUYE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "limit_clusters = redistribution(clusters)"
      ],
      "metadata": {
        "id": "4z5UAhh6UYC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in limit_clusters:\n",
        "  if isinstance(i, list):\n",
        "    print(\"List length:\", len(i), \"tokens:\", calc_tokens(\"\\n\".join(i)))\n",
        "  else:\n",
        "    print(\"String\")"
      ],
      "metadata": {
        "id": "v3xfdh56GmBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#joining the chunks into the cluster to make a string of all the cluster information\n",
        "#Therefore it can be used by langchain with prompt templates\n",
        "\n",
        "tosendlangchain = [\"\\n\".join(i) for i in limit_clusters]\n",
        "\n",
        "for i in tosendlangchain:\n",
        "  print(calc_tokens(i))"
      ],
      "metadata": {
        "id": "RsLODDRWYdfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utilizing langchain for creating LLM chain and prompt templates"
      ],
      "metadata": {
        "id": "IAhyluRn5w8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "def cluster_summaries(text_chunks_list):\n",
        "  template = \"\"\"As an expert summarizer, produce a concise yet comprehensive summary of the given text,\n",
        "  whether it's an article, blog post, conversation, or passage without adding your interpretations.\n",
        "  Your summary should exhibit great detail, depth, and complexity while ensuring clarity and conciseness.\n",
        "  Employ the following content to create the summary:\n",
        "  {text}\"\"\"\n",
        "  prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
        "  llm_chain = LLMChain(prompt=prompt, llm=LLM)\n",
        "\n",
        "  ip = [{'text': i} for i in text_chunks_list]\n",
        "  summary = llm_chain.apply(ip)\n",
        "\n",
        "  return [i['text'] for i in summary]\n",
        "\n",
        "\n",
        "def generate_final_summary(text_chunk): #Takes the final text and generates the final summary\n",
        "  template = \"\"\"I want you to act as a text summarizer to help me create a brief understandable summary of the text I provide,\n",
        "  whether it's an article, blog post, conversation, or passage.\n",
        "  The summary should be in good length, expressing the points and concepts written in the original text without adding your interpretations.\n",
        "  Breifly summarize the following text for me:\n",
        "  {text}\"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
        "  llm_chain = LLMChain(prompt=prompt, llm=LLM)\n",
        "\n",
        "  summary = llm_chain.run(text_chunk)\n",
        "\n",
        "  return summary"
      ],
      "metadata": {
        "id": "olfwa3o_WvhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recursive_summary(listofclusters):\n",
        "  tokens = calc_tokens(\"\\n\".join(listofclusters))\n",
        "  print(\"current tokens:\", tokens)\n",
        "\n",
        "  if tokens+90 <= 4096:\n",
        "    return generate_final_summary(\"\\n\".join(listofclusters)) #proceed to the final summary token limit is not exceeded\n",
        "  else:\n",
        "    summaryofclusters = cluster_summaries(listofclusters) #else use mapreduce like technique\n",
        "    print(summaryofclusters,\"\\n\\n\\n\")\n",
        "    return recursive_summary(summaryofclusters)"
      ],
      "metadata": {
        "id": "W2mf2GCfWvfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = recursive_summary(tosendlangchain)"
      ],
      "metadata": {
        "id": "a6K9mzhVWvdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "id": "YtTSNanEWvbB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}