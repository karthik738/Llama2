{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c415096c-5328-451e-9e3e-c94748d81783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117 --upgrade\n",
    "%pip install -q langchain einops accelerate transformers bitsandbytes scipy\n",
    "%pip install -q xformers sentencepiece sentence-transformers pypdf\n",
    "%pip install -q llama-index==0.7.21 llama_hub==0.0.19 faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca02831-5a10-43d7-b6c9-715b4cacdcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"EDENAI_API_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjoiNTM3ZWE2NWItZTZmZi00ZjQ3LThmY2QtNzU5NDg1YmRhNDMzIiwidHlwZSI6ImFwaV90b2tlbiJ9.NujKgo_tyy5V5SSP78F3s4_vY2Ll9afE578RAaSxKZ8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "436543b5-5ba6-45ec-99f6-f6386d0b364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import EdenAI\n",
    "\n",
    "llm=EdenAI(provider=\"openai\",model=\"text-davinci-003\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53ef636f-3e89-4fa1-8cd8-b2572d85b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from glob import glob\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b92e408b-e547-4896-985f-dc395fec1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def clean_document_nemerics(text):  #Cleans the text\n",
    "  return re.sub(r'(?:\\b|(?<=\\s))\\w{1}\\b|[\\d\\W]+', ' ', text).lower()\n",
    "\n",
    "def load_documents(directory): #Takes the docs directory and returns documents\n",
    "  documents = []\n",
    "  for item_path in glob(directory + \"*.pdf\"):\n",
    "      loader = PyPDFLoader(item_path)\n",
    "      documents.extend(loader.load())\n",
    "  return documents\n",
    "\n",
    "def text_to_chunks(full_string): #Takes full text and divides them into chunks, references, and gives k\n",
    "  full_string = clean_document_nemerics(full_string)\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap = 128)\n",
    "  chunks = text_splitter.split_text(full_string)\n",
    "  references = []\n",
    "\n",
    "  number_of_chunks = len(chunks)\n",
    "  print(\"No of docs: \", number_of_chunks)\n",
    "  number_of_references = math.ceil(number_of_chunks/5)\n",
    "  print(\"No of refenrence documents needed:\", number_of_references)\n",
    "  index_of_reference = number_of_chunks // (number_of_chunks/5)\n",
    "  print(\"Document index:\", index_of_reference)\n",
    "\n",
    "  for i in range(0, number_of_chunks):\n",
    "      if i % 5 == 0:\n",
    "          references.append(chunks[i])\n",
    "          # print(i)\n",
    "\n",
    "  return chunks, references, int(index_of_reference+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bde95f7-a9a6-438b-b385-23d3603b2aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"p50k_base\")\n",
    "\n",
    "def get_similar_chunks(db, reference, k):  #Takes DB, reference, k and give reference documents\n",
    "    chunks = [i.page_content for i in (db.similarity_search(reference, k = k))]\n",
    "    return chunks\n",
    "    \n",
    "def calc_tokens(chunks): #Takes the text and gives the number of tokens\n",
    "    cur_token = len(encoding.encode(chunks))\n",
    "    return cur_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4b344991-c36e-405e-9c37-145cd6f3c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "def generate_summaries(text_chunks_list): #Takes a list of chunks of [(k similar documents), (), ()..] and give their summaries in a list\n",
    "  template = \"\"\"\n",
    "  As an expert summarizer, produce a concise yet comprehensive summary of the given text,\n",
    "  whether it's an article, blog post, conversation, or passage without adding your interpretations.\n",
    "  Your summary should exhibit great detail, depth, and complexity while ensuring clarity and conciseness.\n",
    "  Employ the following content to create the summary:\n",
    "  {text}\n",
    "  \"\"\"\n",
    "    \n",
    "  prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "  llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "  ip = [{'text': i} for i in text_chunks_list]\n",
    " \n",
    "  summaries = llm_chain.apply(ip)\n",
    "\n",
    "  return [i['text'] for i in summaries]\n",
    "\n",
    "\n",
    "def generate_final_summary(text_chunk): #Takes the final text and generates the final summary\n",
    "  template = \"\"\"\n",
    "  I want you to act as a text summarizer to help me create a brief understandable summary of the text I provide,\n",
    "  whether it's an article, blog post, conversation, or passage.\n",
    "  The summary should be 15-20 sentences in length, expressing the points and concepts written in the original text without adding your interpretations.\n",
    "  Employ the following content to create the summary:\n",
    "  ```{text}```\n",
    "  \"\"\"\n",
    "\n",
    "  prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "  llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "  summary = llm_chain({'text': text_chunk})\n",
    "  # print(\"Final summary: \")\n",
    "  # print(summary)\n",
    "\n",
    "  return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e4bb24a6-44c9-4aeb-90bf-91f11d3c073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path will be given then documents will be taken and converted into text\n",
    "\n",
    "#---the text is then split into chunks, references\n",
    "#---chunks will be stored in DB\n",
    "#---similarity search using references\n",
    "#---summaries\n",
    "#---if token limit exceeded, \n",
    "#--------recursively continue the same \n",
    "#---else\n",
    "#--------final summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c45c4449-ef38-4052-867c-612c3787e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurive_summarization(chunks_as_text):\n",
    "    tokens = calc_tokens(chunks_as_text)\n",
    "\n",
    "    #If the token limit is not exceeded. proceed for the final summary\n",
    "    if tokens+1000 < 4096:              \n",
    "        print(\"Current tokens: \", tokens)\n",
    "        return generate_final_summary(chunks_as_text)\n",
    "\n",
    "    #Recursively repeat the process\n",
    "    else: \n",
    "        print(\"Current tokens: \", tokens)\n",
    "\n",
    "        #Text divided into chunks, references, and K\n",
    "        full_text_turned_into_chunks, references, k = text_to_chunks(chunks_as_text)\n",
    "\n",
    "        #Storing the chunks into FAISS using embeddings\n",
    "        db = FAISS.from_texts(full_text_turned_into_chunks, embeddings)\n",
    "\n",
    "        #Retrieving the Similar chunks using references\n",
    "        similar_chunk_k5 = []\n",
    "        for reference in references:\n",
    "            similar_k_texts = get_similar_chunks(db, reference, k)\n",
    "            similar_chunk_k5.append(\". \".join(similar_k_texts))\n",
    "        \n",
    "        summaries = generate_summaries(similar_chunk_k5)\n",
    "        return recurive_summarization(\".\".join(summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "304a5bdc-210c-4f23-8099-5ff6a8b34a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path is given documents and text is extracted\n",
    "documents = load_documents(\"./KG/docs/\")\n",
    "documents_text = [i.page_content for i in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "901338df-d5cb-4f68-90fd-cb6cb8239c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8652ac46-5708-427f-90f3-cf9bb8ffb0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current tokens:  20608\n",
      "No of docs:  167\n",
      "No of refenrence documents needed: 34\n",
      "Document index: 5.0\n",
      "Current tokens:  8512\n",
      "No of docs:  106\n",
      "No of refenrence documents needed: 22\n",
      "Document index: 5.0\n",
      "Current tokens:  6164\n",
      "No of docs:  74\n",
      "No of refenrence documents needed: 15\n",
      "Document index: 4.0\n",
      "Current tokens:  3798\n",
      "No of docs:  49\n",
      "No of refenrence documents needed: 10\n",
      "Document index: 4.0\n",
      "Current tokens:  2302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hs/rv3zghnx695_xtmmj08lh0vc0000gn/T/ipykernel_1053/2676286876.py:1: RuntimeWarning: coroutine 'recurive_summarization' was never awaited\n",
      "  summary = recurive_summarization(\". \".join(documents_text))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "summary = recurive_summarization(\". \".join(documents_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "746c2c76-0cf6-4513-9399-520f04479f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Neats is an extraction-based multi-document summarization system that uses techniques such as term frequency, sentence position, stigma words, term clustering, buddy system of paired sentences, and explicit time annotation to improve topic coverage and readability. It was evaluated on the large-scale summarization evaluation DUC and was among the best performers, performing better on longer summaries and words based on weighted retention than on shorter ones. The Document Understanding Conference (DUC) and Text Summarization Challenge (TSC) are large-scale summarization projects in the US and Japan, respectively. They compile standard training and test collections and provide common and large-scale evaluations in single and multiple document summarization. Common metrics such as pseudo precision and weighted retention are used to measure the effectiveness of automatic summarization systems. Lead sentences are good summary sentence candidates and one needs to cover all documents in a topic to achieve reasonable performance in multi-document summarization. Information such as term frequency and sentence position are used to rank sentences related to the topic of the cluster. The African National Congress (ANC) in South Africa suspended its year-long armed struggle against the white minority government, paving the way for negotiations over a new constitution based on black-white power sharing. The ANC seeks a one-man-one-vote majority rule system, while the government insists on constitutional protection of the rights of minorities, including the whites. The ANC also seeks the release of political prisoners, an end to political trials and executions, and a commitment to free political discussion. This paper discusses the development of a multi-document summarization system designed to produce summaries that emphasize relevant novelty. It builds on previous work in single document summarization by using additional available techniques such as sampling, clustering, and relevance assessments. The system includes a natural language generation component to create cohesive, readable summaries. Challenges on summarization systems are greater in true IR or topic detection contexts and can be addressed by using redundancy and document relevance. Karen Kukich presented a new revision-based model for summary generation. Evaluation results were released in August and the data sets will allow researchers to measure the effects of features on multi-document summarization quality. The query-relevant multi-document summarization system produces the same results when documents are added to a set of other topics. Approaches to multi-document summarization include comparing templates filled in with marginal relevance, which is a linear combination of relevance and novelty to select passages for inclusion in the summary. Large-scale IR and summarization have not yet been truly integrated, and the functionality requirements for multi-document summarization systems are greater in true IR or topic detection contexts. Such systems should be able to permit the user to view related passages to the query and include natural language understanding and generation components.\n"
     ]
    }
   ],
   "source": [
    "print(summary['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc71567-1cf7-4f6d-abe7-d0b9d0ce582d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed51a3-ce05-4d59-90c2-f92b176e0c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beed23c-6e1a-431a-8c50-2038e8c7f548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60411341-2f5a-4b98-ba00-92ccb5fe75a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecd0b49-5d55-447a-b27d-cf46b53426dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43a91533-ff26-4b18-8bc8-fc53f5d18d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of docs:  23\n",
      "No of refenrence documents needed: 5\n",
      "Document index: 5.0\n"
     ]
    }
   ],
   "source": [
    "#Text divided into chunks, references, and K\n",
    "chunks, references, k = text_to_chunks(\". \".join(documents_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9932a81-a01e-4d40-926d-9e01d8ef49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the chunks into FAISS using embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "db = FAISS.from_texts(chunks, embeddings)\n",
    "#db.save_local('faissdb/', 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "acef361c-ab73-44f8-90eb-3ab50be7c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving the Similar chunks using references\n",
    "similar_chunk_k5 = []\n",
    "for reference in references:\n",
    "    similar_k_texts = get_similar_chunks(db, reference, k)\n",
    "    similar_chunk_k5.append(\". \".join(similar_k_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d99d24-b9c6-4204-9898-02b73d9827f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "recurive_summarization(\".. \".join(similar_chunk_k5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
